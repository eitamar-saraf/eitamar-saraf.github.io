---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout>
  <main class="max-w-6xl mx-auto py-12 px-4">
    <h1 class="text-3xl font-bold mb-3">BatchNorm vs No‑BatchNorm: a practical, visual guide</h1>
    <p class="mb-4 text-gray-700">Batch Normalization (BN) is one of those ideas everyone has heard “just works.” In this post, we’ll build an intuition for why—starting from the data and model, then digging into signals inside the network. Along the way, we’ll look at concrete plots that show how BN changes training dynamics.</p>

    <h2 class="text-2xl font-semibold mt-6 mb-2">TL;DR</h2>
    <ul class="list-disc list-inside mb-6 text-gray-700">
      <li>BN normalizes layer activations (zero mean, unit variance) per batch, then learns a scale (γ) and shift (β) so the network can still represent rich functions.</li>
      <li>This keeps signals in healthy ranges, makes gradients more stable, and allows higher learning rates without divergence.</li>
      <li>With BN, validation accuracy rises faster and higher; without BN, training can be slower, brittle at higher LR, and more sensitive to initialization.</li>
      <li>BN is most effective with reasonable batch sizes; for very small batches, consider alternatives like LayerNorm/GroupNorm or tuning momentum/statistics.</li>
    </ul>

    <h2 class="text-2xl font-semibold mt-8 mb-2">The setup at a glance</h2>
    <div class="mb-6 text-gray-700 space-y-2">
      <p><b>Dataset.</b> CIFAR‑10 (10 classes, 32×32 color images). We use a standard train/val split with light augmentation (random crop/flip). The goal isn’t SOTA—it’s to probe training behavior.</p>
      <p><b>Network.</b> A compact CNN with 5 conv blocks: Conv → ReLU (and BN in the BN variant), then a small classifier head. “Block 1…5” in the plots refer to these conv blocks from shallow to deep.</p>
      <p><b>Training.</b> Same initialization, optimizer, and schedule for both variants. We sweep two learning rates (0.02 and 0.2) to show stability differences. Any gaps we see are largely due to BN.</p>
      <p class="text-sm text-gray-600">Why this setup? Small, fast to run, and complex enough to show real effects like dead neurons, gradient scale drift, and batch‑to‑batch shifts.</p>
    </div>

    <h2 class="text-2xl font-semibold mt-8 mb-2">What BatchNorm actually does</h2>
    <p class="mb-6 text-gray-700">For each channel in a layer, BN recenters and rescales the activations of the current mini‑batch to have mean ≈ 0 and std ≈ 1, then applies a learned scale <i>γ</i> and shift <i>β</i>. During training it uses per‑batch stats; at inference it uses running averages collected during training. This keeps signals in a “friendly” numeric range, so gradients neither vanish nor explode easily. The optimizer sees a smoother landscape and tolerates larger learning rates.</p>
    <div class="mb-6 p-3 rounded border bg-amber-50 text-amber-900 text-sm">
      <b>Heads‑up:</b> With very small batches, the batch statistics can be noisy. In those cases, consider GroupNorm or LayerNorm, or tune BN momentum and learning rate.
    </div>

    <h2 class="text-2xl font-semibold mt-8 mb-2">First, the big picture: does BN help?</h2>
    <figure class="bg-white rounded border p-2 mb-2">
      <img src="/plots/bn/val acc compare.png" alt="Validation accuracy comparison BN vs No-BN across learning rates" class="w-full rounded" />
    </figure>
    <p class="text-sm text-gray-600 mb-6">Validation accuracy, BN vs No‑BN, over training (two learning rates). With BN, accuracy climbs faster and higher; without BN, it lags or destabilizes at higher LR.</p>

  <p class="mb-8 text-gray-700">So BN improves validation accuracy. Without BN, the higher LR often overshoots and oscillates; with BN, training is smoother and reaches a better plateau. Now let’s open the hood and see why the optimizer’s job gets easier.</p>

    <h2 class="text-2xl font-semibold mt-8 mb-2">1) Activation scale is tamed</h2>
  <p class="mb-4 text-gray-700">Before ReLU, it’s healthy if activations are centered and reasonably scaled. If means drift or variances explode, gradients either die out or blow up as they travel through layers. BN nudges each batch back toward mean ≈ 0 and std ≈ 1, layer by layer, step by step.</p>
  <p class="mb-3 text-gray-700"><b>How to read these plots:</b> The curves show the mean and std of pre‑ReLU activations over training. Desired behavior: means near zero and stds that settle into a stable band (not collapsing to 0 and not exploding).</p>
    <div class="mb-4">
      <div class="carousel" data-images='["/plots/bn/act mean pre relu block 1.png","/plots/bn/act mean pre relu block 2.png","/plots/bn/act mean pre relu block 3.png","/plots/bn/act mean pre relu block 4.png","/plots/bn/act mean pre relu block 5.png"]' data-captions='["Mean (pre‑ReLU), block 1","Mean (pre‑ReLU), block 2","Mean (pre‑ReLU), block 3","Mean (pre‑ReLU), block 4","Mean (pre‑ReLU), block 5"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Activation mean pre‑ReLU across blocks" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>
    <div class="mb-8">
      <div class="carousel" data-images='["/plots/bn/act std pre relu block 1.png","/plots/bn/act std pre relu block 2.png","/plots/bn/act std pre relu block 3.png","/plots/bn/act std pre relu block 4.png","/plots/bn/act std pre relu block 5.png"]' data-captions='["Std (pre‑ReLU), block 1","Std (pre‑ReLU), block 2","Std (pre‑ReLU), block 3","Std (pre‑ReLU), block 4","Std (pre‑ReLU), block 5"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Activation std pre‑ReLU across blocks" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>
  <p class="mb-8 text-gray-700">With BN, curves are tighter and more stable—gradients propagate predictably. Without BN, curves drift with learning rate and depth (later blocks suffer more), which makes training brittle.</p>

    <h2 class="text-2xl font-semibold mt-8 mb-2">2) Fewer dead activations</h2>
  <p class="mb-4 text-gray-700">When pre‑ReLU activations live far below zero, ReLU outputs are zero and gradients stop flowing—neurons “die.” BN keeps the distribution centered, so a healthy fraction remains active.</p>
  <p class="mb-3 text-gray-700"><b>How to read these plots:</b> We track the fraction of channels/units that are inactive after ReLU. Lower is usually better, but some sparsity is fine. Watch how high LR without BN spikes the inactive fraction.</p>
    <div class="mb-8">
      <div class="carousel" data-images='["/plots/bn/fraction inactive pre relu block 1.png","/plots/bn/fraction inactive pre relu block 2.png","/plots/bn/fraction inactive pre relu block 3.png","/plots/bn/fraction inactive pre relu block 4.png","/plots/bn/fraction inactive pre relu block 5.png"]' data-captions='["Inactive fraction, block 1","Inactive fraction, block 2","Inactive fraction, block 3","Inactive fraction, block 4","Inactive fraction, block 5"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Fraction inactive pre‑ReLU across blocks" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>

    <h2 class="text-2xl font-semibold mt-8 mb-2">3) Gradients are steadier</h2>
  <p class="mb-4 text-gray-700">BN narrows the spread of gradient norms across layers. That stability is a big reason you can crank the learning rate without divergence. If gradients swing wildly between layers, updates become inconsistent.</p>
  <p class="mb-3 text-gray-700"><b>How to read these plots:</b> Steady curves with similar scale across blocks are good. Big spikes or tiny near‑zero gradients are red flags (exploding or vanishing signals).</p>
    <div class="mb-8">
      <div class="carousel" data-images='["/plots/bn/grad norm block 1.png","/plots/bn/grad norm block 2.png","/plots/bn/grad norm block 3.png","/plots/bn/grad norm block 4.png","/plots/bn/grad norm block 5.png","/plots/bn/grad norm head.png"]' data-captions='["Grad norm, block 1","Grad norm, block 2","Grad norm, block 3","Grad norm, block 4","Grad norm, block 5","Grad norm, head"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Gradient norms across blocks" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>

    <h2 class="text-2xl font-semibold mt-8 mb-2">4) Updates are sized more sanely</h2>
  <p class="mb-4 text-gray-700">Relative update size, ||ΔW|| / ||W||, tells you how aggressively weights change compared to their current magnitude. Too small and learning stalls; too large and training thrashes. BN keeps these updates in a more consistent range across depth.</p>
  <p class="mb-3 text-gray-700"><b>How to read these plots:</b> Look for consistent, moderate curves across blocks. With no BN at high LR, you’ll often see spikes in deeper layers or the head—signs of unstable steps.</p>
    <div class="mb-8">
      <div class="carousel" data-images='["/plots/bn/rel update size block 1.png","/plots/bn/rel update size block 2.png","/plots/bn/rel update size block 3.png","/plots/bn/rel update size block 4.png","/plots/bn/rel update size block 5.png","/plots/bn/rel update size head.png"]' data-captions='["Rel. update size, block 1","Rel. update size, block 2","Rel. update size, block 3","Rel. update size, block 4","Rel. update size, block 5","Rel. update size, head"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Relative update size across blocks" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>

    <h2 class="text-2xl font-semibold mt-8 mb-2">5) Less batch‑to‑batch whiplash</h2>
  <p class="mb-4 text-gray-700">One way to quantify “internal covariate shift” is to track how much activations shift from one mini‑batch to the next. High shifts mean the optimizer keeps chasing a moving target. BN dampens this, especially at higher learning rates.</p>
  <p class="mb-3 text-gray-700"><b>How to read these plots:</b> Compare BN vs No‑BN for the same LR. The BN curves should be lower and smoother, meaning the distribution the next layer sees isn’t jumping around as much.</p>
    <div class="mb-8">
      <div class="carousel" data-images='["/plots/bn/batch to batch act shift bn-lr0.02.png","/plots/bn/batch to batch act shift Nobn-lr0.02.png","/plots/bn/batch to batch act shift bn-lr0.2.png","/plots/bn/batch to batch act shift Nobn-lr0.2.png"]' data-captions='["BN, lr=0.02","No‑BN, lr=0.02","BN, lr=0.2","No‑BN, lr=0.2"]'>
        <div class="relative bg-white rounded border overflow-hidden">
          <img class="carousel-img w-full rounded" alt="Batch‑to‑batch activation shift" />
          <button class="carousel-prev absolute top-1/2 -translate-y-1/2 left-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">‹</button>
          <button class="carousel-next absolute top-1/2 -translate-y-1/2 right-2 bg-white/80 hover:bg-white rounded-full px-3 py-1 shadow border">›</button>
        </div>
        <div class="mt-2 text-xs text-gray-600 carousel-caption"></div>
      </div>
    </div>

    <div class="mb-8 p-3 rounded border bg-blue-50 text-blue-900 text-sm">
      <b>Bottom line:</b> BN normalizes activations so gradients flow better, updates are well‑scaled, and batch‑to‑batch noise is reduced. Optimizers see a smoother problem, so you get faster, more reliable training—often at higher learning rates.
    </div>

    <div class="mt-8 flex items-center gap-3">
      <a href="/demos/batchnorm" class="inline-flex items-center gap-2 px-4 py-2 rounded bg-blue-600 text-white hover:bg-blue-700">Explore the interactive dashboard</a>
      <span class="inline-flex items-center gap-2 px-4 py-2 rounded border text-blue-700 bg-gray-50">Repository: coming soon</span>
    </div>

    <p class="mt-6 text-sm text-gray-600">Plots above come directly from the same runs as the dashboard. For more context, open the demo and interact with the panels.</p>

    <script type="module">
      function initCarousel(el){
        const imgs = JSON.parse(el.getAttribute('data-images') || '[]');
        const caps = JSON.parse(el.getAttribute('data-captions') || '[]');
        const imgEl = el.querySelector('.carousel-img');
        const capEl = el.querySelector('.carousel-caption');
        const prev = el.querySelector('.carousel-prev');
        const next = el.querySelector('.carousel-next');
        let i = 0;
        function render(){
          imgEl.src = imgs[i];
          if (capEl) capEl.textContent = caps[i] || '';
        }
        prev?.addEventListener('click', ()=>{ i = (i - 1 + imgs.length) % imgs.length; render(); });
        next?.addEventListener('click', ()=>{ i = (i + 1) % imgs.length; render(); });
        // Keyboard support when focused
        el.setAttribute('tabindex','0');
        el.addEventListener('keydown', (e)=>{
          if(e.key==='ArrowLeft'){ i = (i - 1 + imgs.length) % imgs.length; render(); }
          if(e.key==='ArrowRight'){ i = (i + 1) % imgs.length; render(); }
        });
        render();
      }
      document.querySelectorAll('.carousel').forEach(initCarousel);
    </script>
  </main>
</BaseLayout>
